{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Number of GPUs:\", len(gpus))\n",
    "for gpu in gpus:\n",
    "    print(\"GPU Name:\", gpu.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a perceptron?\n",
    "======================\n",
    "A perceptron is a single layer neural network. It is a linear classifier - although it can be used for regression as well. In this post, we will see how to implement a perceptron for classification in Python.\n",
    "\n",
    "Perceptron\n",
    "----------\n",
    "A perceptron has one or more inputs, a bias, an activation function, and a single output. The perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output. There are many possible activation functions to choose from, such as the logistic function, a trigonometric function, a step function etc. We will use the step function as our activation function.\n",
    "\n",
    "![Perceptron](https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/images/nn1.png)\n",
    "\n",
    "The above figure shows a perceptron with 3 inputs.\n",
    "\n",
    "The output of a perceptron is binary, either 0 or 1, since the output of the step function is binary. We can think of the perceptron as a model of a neuron. It either fires or it doesn't.\n",
    "\n",
    "The perceptron can be used for supervised learning. That is, it can be trained using a set of inputs where the desired output is known. The perceptron will then try to learn a decision boundary that correctly classifies the inputs. Inputs could be real-valued, boolean-valued, or even discrete-valued, but they must be able to be converted to real-valued inputs. The weights, and the bias, must be real numbers.\n",
    "\n",
    "The training algorithm is as follows:\n",
    "\n",
    "1. Initialize the weights and bias to 0 or small random numbers\n",
    "2. For each training sample x(i):\n",
    "    1. Calculate the output value y\n",
    "    2. Update the weights\n",
    "3. Repeat step 2 until the algorithm converges\n",
    "\n",
    "The output value y is calculated as follows:\n",
    "\n",
    "![Output](https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/images/nn2.png)\n",
    "\n",
    "The update rule for the weights is as follows:\n",
    "\n",
    "![Update](https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/images/nn3.png)\n",
    "\n",
    "The perceptron learning algorithm is guaranteed to converge if the training set is linearly separable. If not, the algorithm will never stop running, since there will always be at least one misclassified sample, and the weights will keep getting updated. In practice, we can set a maximum number of iterations and stop after that.\n",
    "\n",
    "Example\n",
    "-------\n",
    "Let's see how to implement a perceptron for the OR function. The OR function takes two inputs, either 0 or 1, and returns 1 if one of the inputs is 1, and 0 if both are 0. The truth table for the OR function is as follows:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 1 |\n",
    "\n",
    "We can see that the OR function is linearly separable. We can draw a straight line that separates the inputs that return 0 from the inputs that return 1. The perceptron will learn this line.\n",
    "\n",
    "We will use the following values for the weights and the bias:\n",
    "\n",
    "| w1 | w2 | b |\n",
    "|----|----|---|\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "We will use the step function as our activation function. The step function returns 1 if the input is greater than or equal to 0, and 0 otherwise.\n",
    "\n",
    "The output of the perceptron is calculated as follows:\n",
    "\n",
    "![Output](https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/images/nn4.png)\n",
    "\n",
    "The output of the perceptron is 0 for the first two inputs, and 1 for the last two inputs. This is the same as the OR function.\n",
    "\n",
    "We will use the following inputs for training the perceptron:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 1 |\n",
    "\n",
    "We will use the perceptron to learn the OR function. We will use the perceptron learning algorithm to train the perceptron. We will use a learning rate of 0.01. We will use a maximum of 10 iterations.\n",
    "\n",
    "The weights and bias are initialized to 0. The output of the perceptron is calculated for each input. If the output is not equal to the desired output, the weights and bias are updated. This process is repeated until the algorithm converges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Problem\n",
    "\n",
    "The XOR problem is a simple problem in which the goal is to learn the XOR function. The XOR function takes two inputs, either 0 or 1, and returns 1 if one of the inputs is 1, and 0 if both are 0 or both are 1. The truth table for the XOR function is as follows:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "We can see that the XOR function is not linearly separable. We cannot draw a straight line that separates the inputs that return 0 from the inputs that return 1. The perceptron cannot learn this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[0.514573  ]\n",
      " [0.5792802 ]\n",
      " [0.42871475]\n",
      " [0.46079195]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the XOR inputs and outputs\n",
    "inputs = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
    "outputs = tf.constant([[0], [1], [1], [0]], dtype=tf.float32)\n",
    "\n",
    "# Define the neuron model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=2, activation='sigmoid', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(inputs, outputs, epochs=1000, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.predict(inputs)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
